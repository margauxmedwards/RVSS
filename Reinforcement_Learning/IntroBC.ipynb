{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gIwkMy9WnrRn"
   },
   "source": [
    "# <center> Learning to Act - Session 1</center>\n",
    "\n",
    "Introduction to Policy Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L72xzKhojEmn"
   },
   "source": [
    "# Activity 0. Setup\n",
    "\n",
    "#### Install additional dependencies\n",
    "\n",
    "Let us first make sure that all the required dependencies are installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JlrEctSbjEmp"
   },
   "outputs": [],
   "source": [
    "#removing this while developing the notebook from drive\n",
    "#!git clone https://github.com/rvss-australia/RVSS.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21010,
     "status": "ok",
     "timestamp": 1770094588437,
     "user": {
      "displayName": "Dana Kulić",
      "userId": "03890108134669700774"
     },
     "user_tz": -660
    },
    "id": "cqFBDmfZRZQr",
    "outputId": "e72ff53f-ce38-4c25-e607-c5ba3ecbfdc1"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "%cd /content/drive/MyDrive/projects/RVSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1770094591987,
     "user": {
      "displayName": "Dana Kulić",
      "userId": "03890108134669700774"
     },
     "user_tz": -660
    },
    "id": "T-4opaDRjEmr",
    "outputId": "31ea9855-54cf-4bd8-ebd3-72abc42e6b97"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir(\"Reinforcement_Learning/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3625,
     "status": "ok",
     "timestamp": 1770094599437,
     "user": {
      "displayName": "Dana Kulić",
      "userId": "03890108134669700774"
     },
     "user_tz": -660
    },
    "id": "IdctBV7FjEmo",
    "outputId": "90836d08-5e77-4d8f-8ad6-ecc7b1bba7a0"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FY8sb3WLnrT6"
   },
   "source": [
    "#### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1847,
     "status": "ok",
     "timestamp": 1770094606277,
     "user": {
      "displayName": "Dana Kulić",
      "userId": "03890108134669700774"
     },
     "user_tz": -660
    },
    "id": "xHhQP_yrnrUA"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from RL_Support.gym_simple_gridworlds.helper import *\n",
    "from RL_Support.gym_simple_gridworlds.envs.grid_env import GridEnv\n",
    "from RL_Support.gym_simple_gridworlds.envs.grid_2dplot import *\n",
    "\n",
    "from collections import namedtuple, defaultdict\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "UP = 0; DOWN = 1; LEFT = 2; RIGHT = 3; STAY = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTi7z5gbnrWG"
   },
   "source": [
    "# Activity 1. Elements of an MDP (Grid World Example)\n",
    "\n",
    "A humble robot living in grid world\n",
    "\n",
    "![GridWorldExample.png](https://i.postimg.cc/5tMM5vqf/Grid-World-Example.png)\n",
    "\n",
    "- The states $s \\in \\mathcal{S}$ correspond to locations in the grid. Each location has also a cell index associated to it, e.g., cell index 4 is associated to location (row=1,col=0)\n",
    "- The robot can move up, down, left, or right. Actions correpond to unit increments or decrements in the specified direction.\n",
    "    - Up : (-1,0)\n",
    "    - Down: (1,0)\n",
    "    - Left: (0,-1)\n",
    "    - Right: (0, 1)\n",
    "- Each action is represented by a number. Action (Up) is represented by 0, (Down) by 1, (Left) by 2 and, finally, (Right) by 3. No actions are available at a terminal state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjXoXqOnnrXQ"
   },
   "source": [
    "## Create Environment and Explore its Attributes\n",
    "\n",
    "The noise parameter corresponds to the probability of a change of direction when an action is taken (e.g., going left/right when agent decides to move up/down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1770094619549,
     "user": {
      "displayName": "Dana Kulić",
      "userId": "03890108134669700774"
     },
     "user_tz": -660
    },
    "id": "iv58f67EnrXZ"
   },
   "outputs": [],
   "source": [
    "# Create a Grid World instance\n",
    "grid_world = GridEnv(gamma=0.9, noise=0.2, living_reward=-0.04)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zzBYFM7WnrXb"
   },
   "source": [
    "### State and Action Spaces\n",
    "\n",
    "Let's take a look at the state and action spaces of our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1770094624377,
     "user": {
      "displayName": "Dana Kulić",
      "userId": "03890108134669700774"
     },
     "user_tz": -660
    },
    "id": "BJhhxR-HnrXw",
    "outputId": "7f535ded-2757-42e3-9a45-ee948815a391"
   },
   "outputs": [],
   "source": [
    "# State (or observation) space\n",
    "print(grid_world.observation_space)\n",
    "print(grid_world.get_states())\n",
    "print()\n",
    "\n",
    "# Action space\n",
    "print(grid_world.action_space)\n",
    "print(grid_world.get_actions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwWFnDIKnrX8"
   },
   "source": [
    "### Transition Function\n",
    "\n",
    "Let's take a look at the current state transition function. Some things to keep in mind regarding the transition function:\n",
    "\n",
    "1. Given that $\\mathcal{T}: \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow \\mathbb{R}$, the ``state_transitions`` attribute of the class ``GridEnv`` corresponds to a 3-Dimensional numpy array of size $11\\times4\\times11$.\n",
    "2. With a noise attribute set to 0.2, at state 5, if the agent chooses to move up, it will end up at:\n",
    "    - state 2 with 80% probability,\n",
    "    - state 6 with 10% probability, or\n",
    "    - state 5 with 10% probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1770094635671,
     "user": {
      "displayName": "Dana Kulić",
      "userId": "03890108134669700774"
     },
     "user_tz": -660
    },
    "id": "cGOVNUHQnrYE",
    "outputId": "0ae29e7f-e9e6-48f1-9314-5bd12d071d4f"
   },
   "outputs": [],
   "source": [
    "# at state 5 the agent takes action 0 (going up)\n",
    "print(grid_world.state_transitions[5,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Puxe1hrynrYK"
   },
   "source": [
    "### Policy\n",
    "\n",
    "Let's see the path of an agent moving on our grid world according to a policy $\\pi$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "executionInfo": {
     "elapsed": 160,
     "status": "ok",
     "timestamp": 1770094716873,
     "user": {
      "displayName": "Dana Kulić",
      "userId": "03890108134669700774"
     },
     "user_tz": -660
    },
    "id": "pkCYTTSSnrYM",
    "outputId": "118b13c7-209f-495d-df89-404b25ba783a"
   },
   "outputs": [],
   "source": [
    "# We represent a policy as a 2-Dimensional numpy array\n",
    "policy_matrix = np.array([[RIGHT,  RIGHT,  RIGHT,  -1],\n",
    "                          [UP, STAY,  UP,  -1],\n",
    "                          [UP, LEFT,  UP,   LEFT]])\n",
    "\n",
    "policy_matrix2 = np.array([[RIGHT,  RIGHT,  UP,  -1],\n",
    "                          [UP, STAY,  UP,  -1],\n",
    "                          [UP, LEFT,  UP,   LEFT]])\n",
    "\n",
    "plot_policy(grid_world, policy_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qw1wo7-0nrYV"
   },
   "source": [
    "Let's now apply this policy and observe the agent's behavior (blue dot in the figure shown below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 498
    },
    "executionInfo": {
     "elapsed": 2279,
     "status": "ok",
     "timestamp": 1770094778662,
     "user": {
      "displayName": "Dana Kulić",
      "userId": "03890108134669700774"
     },
     "user_tz": -660
    },
    "id": "DZ84dyGjnrYd",
    "outputId": "3a154b9d-f2dc-45fd-c78c-a27b81232b68"
   },
   "outputs": [],
   "source": [
    "# Create a Grid World instance\n",
    "grid_world = GridEnv(gamma=0.9, noise=0.2, living_reward=-0.04)\n",
    "s_x, s_y = get_state_to_plot(grid_world)\n",
    "\n",
    "# We can visualize our grid world using the render() function\n",
    "fig, ax = grid_world.render()\n",
    "agent, = ax.plot([], [], 'o', color='b', linewidth=6)\n",
    "reward_text = ax.text(0.02, 0.95, '', transform=ax.transAxes)\n",
    "\n",
    "done = False\n",
    "cumulative_reward = 0\n",
    "cur_state = grid_world.cur_state\n",
    "path_to_plot = []\n",
    "\n",
    "while not done:\n",
    "    _, cur_reward, done, _ = grid_world.step(int(policy_matrix[cur_state[0], cur_state[1]]))\n",
    "    cur_state = grid_world.cur_state\n",
    "    n_x, n_y = get_state_to_plot(grid_world)\n",
    "    cumulative_reward += cur_reward\n",
    "    path_to_plot.append([cumulative_reward, n_x, n_y])\n",
    "\n",
    "def init():\n",
    "    agent.set_data([s_x + 0.5], [s_y + 0.5])\n",
    "    reward_text.set_text('')\n",
    "    return agent, reward_text\n",
    "\n",
    "def animate(i):\n",
    "    if i < len(path_to_plot):\n",
    "        r, n_x, n_y = path_to_plot[i]\n",
    "        agent.set_data([n_x + 0.5], [n_y + 0.5])\n",
    "        reward_text.set_text('Cumulative reward: %.2f' % r)\n",
    "    return agent, reward_text\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, frames=len(path_to_plot), blit=False, interval=500, init_func=init,\n",
    "                              repeat=False)\n",
    "\n",
    "plt.close('all')\n",
    "display(HTML(f\"<div align=\\\"center\\\">{ani.to_jshtml()}</div>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ZHfqaQAjEmy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NbfXJD67EMI"
   },
   "source": [
    "# Activity 2. Behavioural Cloning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DdehGAv7NRK"
   },
   "source": [
    "We would like to learn this policy from demonstrations.  Let's collect a dataset of demonstrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "executionInfo": {
     "elapsed": 430,
     "status": "ok",
     "timestamp": 1770095069304,
     "user": {
      "displayName": "Dana Kulić",
      "userId": "03890108134669700774"
     },
     "user_tz": -660
    },
    "id": "pj7r4fZX_KHb",
    "outputId": "9120b997-61ae-4cf5-c48d-875f446df28c"
   },
   "outputs": [],
   "source": [
    "Sample = namedtuple('Sample', ['state', 'action', 'reward', 'next_state'])\n",
    "grid_world = GridEnv(gamma=0.9, noise=0.0, living_reward=-0.04, width = 4, height = 3)\n",
    "\n",
    "\n",
    "# We represent the expert's policy as a 2-Dimensional numpy array\n",
    "expert_policy = np.array([[RIGHT, RIGHT,  RIGHT,  STAY],\n",
    "                          [UP,    STAY,   UP,     STAY],\n",
    "                          [UP,    LEFT,   UP,     LEFT]])\n",
    "\n",
    "plot_policy(grid_world, expert_policy)\n",
    "\n",
    "\n",
    "# We allow for the expert demonstration to be noisy\n",
    "def get_noisy_policy(env, state, policy, noise):\n",
    "    x, y = np.argwhere(env.grid == state)[0]\n",
    "    optimal_action = int(policy[x, y])\n",
    "    if np.random.rand() < noise:\n",
    "        action = np.random.choice([a for a in env.get_actions() if a != optimal_action])\n",
    "    else:\n",
    "        action = optimal_action\n",
    "    return action\n",
    "\n",
    "# A helper function to generate one demonstration (one episode)\n",
    "def generate_noisy_policy_episode(env, policy, initial_state, noise):\n",
    "\n",
    "    # handle initial_state input, can accept both tuple or integer\n",
    "    if isinstance(initial_state, int):\n",
    "        if initial_state in grid_world.get_states():\n",
    "            state = initial_state\n",
    "        else:\n",
    "            raise RuntimeError(\"state is invalid\")\n",
    "    elif (isinstance(initial_state, tuple)\n",
    "          and len(initial_state) == 2\n",
    "          and all(isinstance(item, int) for item in initial_state)):\n",
    "        state = int(env.grid[initial_state])\n",
    "    else:\n",
    "        raise TypeError(\"Argument must be an integer or a list of two integers.\")\n",
    "\n",
    "    # Episode generation with specified initial_state\n",
    "    episode = []\n",
    "    env.cur_state = tuple(np.argwhere(env.grid == state)[0])\n",
    "    env.idx_cur_state = state\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = get_noisy_policy(env, state, policy, noise)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        episode.append(Sample(state, action, reward, next_state))\n",
    "        state = next_state\n",
    "\n",
    "    return episode\n",
    "\n",
    "\n",
    "# Generate the epsiodes\n",
    "N_episodes = 100\n",
    "episodes = []\n",
    "for i in range(N_episodes):\n",
    "    episodes.append(generate_noisy_policy_episode(env = grid_world,\n",
    "                                                  policy = expert_policy,\n",
    "                                                  initial_state = 0,\n",
    "                                                  noise = 0.0))\n",
    "# for i in range(N_episodes):\n",
    "#     episodes.append(generate_noisy_policy_episode(env = grid_world,\n",
    "#                                                   policy = expert_policy,\n",
    "#                                                   initial_state = 7,\n",
    "#                                                   noise = 0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unQo5db2pR-s"
   },
   "source": [
    "Now we will learn the robot's policy directly from these demonstrations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "executionInfo": {
     "elapsed": 211,
     "status": "ok",
     "timestamp": 1770095142092,
     "user": {
      "displayName": "Dana Kulić",
      "userId": "03890108134669700774"
     },
     "user_tz": -660
    },
    "id": "sZlfN_5G7Vt9",
    "outputId": "028c36b1-64aa-4c24-b458-2f8257ca82aa"
   },
   "outputs": [],
   "source": [
    "def behaviour_cloning(env, episodes):\n",
    "    counted_actions = np.zeros((len(grid_world.get_states()), len(grid_world.get_actions())))\n",
    "    for episode in episodes:\n",
    "        for obs in episode:\n",
    "            counted_actions[obs.state, obs.action] += 1\n",
    "    policy = deepcopy(env.grid)\n",
    "    policy[~np.isnan(policy)] = np.argmax(counted_actions, axis = 1)\n",
    "    return policy, counted_actions\n",
    "\n",
    "policy, counted_actions = behaviour_cloning(grid_world, episodes)\n",
    "plot_policy(grid_world, policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1770095151811,
     "user": {
      "displayName": "Dana Kulić",
      "userId": "03890108134669700774"
     },
     "user_tz": -660
    },
    "id": "Z-yqSSjtBxWz",
    "outputId": "39c12162-201c-4500-c034-70b93086f60b"
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "def plot_policy_stats(env, counted_actions):\n",
    "    height, width = env.grid.shape\n",
    "    hover_text = [[\"\" for _ in range(width)] for _ in range(height)]\n",
    "    data = np.zeros((height, width))\n",
    "    for i in range(height):\n",
    "        for j in range(width):\n",
    "            if env.is_obstacle(i, j):\n",
    "                hover_text[i][j] = \"obstacle\"\n",
    "                data[i, j] = 0\n",
    "            elif env.is_terminal_state(i, j):\n",
    "                hover_text[i][j] = \"terminal_state\"\n",
    "                data[i, j] = 0\n",
    "            else:\n",
    "                state = int(env.grid[i, j])\n",
    "                n_visits = np.sum(counted_actions[state])\n",
    "                hover_text[i][j] = (f\"state: {state}<br>\"\n",
    "                                f\"total visits: {n_visits}<br>\"\n",
    "                                f\"UP: {counted_actions[state][UP]}<br>\"\n",
    "                                f\"DOWN: {counted_actions[state][DOWN]}<br>\"\n",
    "                                f\"LEFT: {counted_actions[state][LEFT]}<br>\"\n",
    "                                f\"RIGHT: {counted_actions[state][RIGHT]}\")\n",
    "                data[i, j] = n_visits\n",
    "\n",
    "    # Create a heatmap for the grid\n",
    "    fig = go.Figure(data=go.Heatmap(\n",
    "        z=data[::-1],\n",
    "        x=np.arange(1, width+1),\n",
    "        y=np.arange(1, height+1),\n",
    "        hoverinfo='text',\n",
    "        text=hover_text[::-1],\n",
    "        showscale=False,  # Hide the color scale bar\n",
    "        colorscale='bluered'  # Color scheme, can be customized\n",
    "    ))\n",
    "\n",
    "    # Update layout to better represent a grid of squares\n",
    "    fig.update_layout(\n",
    "        xaxis=dict(tickmode='linear', showticklabels = False),\n",
    "        yaxis=dict(tickmode='linear', showticklabels = False),\n",
    "        xaxis_showgrid=False,\n",
    "        yaxis_showgrid=False,\n",
    "        height=600,\n",
    "        width=600,\n",
    "        title=\"Heat Map of visited states and actions\"\n",
    "    )\n",
    "\n",
    "    # Preventing axis from being squished\n",
    "    fig.update_xaxes(scaleanchor=\"y\", scaleratio=1)\n",
    "\n",
    "    # Show the figure\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "plot_policy_stats(grid_world, counted_actions)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "https://github.com/rvss-australia/RVSS/blob/main/Reinforcement_Learning/Session%201%20IntroRL.ipynb",
     "timestamp": 1706923474265
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
