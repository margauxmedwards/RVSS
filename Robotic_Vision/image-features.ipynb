{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_0aEaFYzu5L"
      },
      "source": [
        "<table>\n",
        "  <tr>\n",
        "    <td><img src=\"https://github.com/rvss-australia/RVSS/blob/main/Pics/RVSS-logo-col.med.jpg?raw=1\" width=\"400\"></td>\n",
        "    <td><div align=\"left\"><font size=\"30\">Image Features</font></div></td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "(c) Peter Corke 2024\n",
        "\n",
        "Robotics, Vision & Control: Python, see Chapter 12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJjzn-jIzu5O"
      },
      "source": [
        "## Configuring the Jupyter environment\n",
        "We need to import some packages to help us with linear algebra (`numpy`), graphics (`matplotlib`), and machine vision (`machinevisiontoolbox`).\n",
        "If you're running locally you need to have these packages installed.  If you're running on CoLab we have to first install machinevisiontoolbox which is not preinstalled, this will be a bit slow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puZNEjc1zu5O"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    print('Running on CoLab')\n",
        "    !pip install machinevision-toolbox-python\n",
        "    COLAB = True\n",
        "except:\n",
        "    COLAB = False\n",
        "\n",
        "%matplotlib ipympl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "matplotlib.rcParams['figure.figsize'] = [8, 8]\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "from machinevisiontoolbox import Image, Kernel\n",
        "from spatialmath.base import plot_point\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86KrKDQYzu5a"
      },
      "source": [
        "# Harris corner features\n",
        "## From first principles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSN7ZQULzu5a"
      },
      "source": [
        "We now have all the ingredients (and knowledge) needed to find interest points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoHQMxJNzu5b"
      },
      "outputs": [],
      "source": [
        "castle = Image.Read('castle.png', grey=True, dtype='float')\n",
        "castle.disp()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeWq8RK0zu5b"
      },
      "source": [
        "The fundamental intuition behind interest points is that they have a high gradient in two orthogonal directions, but not necessarily the u- and v-axis directions.\n",
        "\n",
        "However we start by computing the gradient in the u- and v-axis directions and then for every pixel compute this matrix\n",
        "\n",
        "$\\mathbf{A} = \\begin{pmatrix} \\mathbf{G}_\\sigma * \\mathbf{I}_u^2  & \\mathbf{G}_\\sigma * \\mathbf{I}_u \\mathbf{I}_v \\\\ \\mathbf{G}_\\sigma * \\mathbf{I}_u \\mathbf{I}_v & \\mathbf{G}_\\sigma * \\mathbf{I}_v^2 \\end{pmatrix}$\n",
        "\n",
        "This $2 \\times 2$ symmetric matrix is called the structure tensor.\n",
        "It captures the intensity structure of the local neighborhood and its eigenvalues provide a rotationally invariant description of the neighborhood. The elements of the $\\mathbf{A}$ matrix are computed from the image gradients, squared or multiplied, and then smoothed using a weighting matrix. The latter reduces noise and improves the stability and reliability of the detector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KN3aO2WBzu5b"
      },
      "outputs": [],
      "source": [
        "# compute derivatives\n",
        "\n",
        "kernel = Kernel.Sobel()\n",
        "\n",
        "Iu = castle.convolve(kernel)\n",
        "Iv = castle.convolve(kernel.T)\n",
        "\n",
        "# make a Gaussian smoothing kernel 11x11 with sigma=1\n",
        "w2 = 5  # half width\n",
        "k = range(-w2, w2+1)\n",
        "sigma = 1\n",
        "[U,V] = np.meshgrid(k, k)\n",
        "kgaussian = 1.0 / (2 * math.pi * sigma**2) * np.exp(-(U**2 + V**2) / (2 * sigma**2))\n",
        "\n",
        "# could also use kgaussian = kgauss(sigma, w2)\n",
        "\n",
        "# compute the 3 unique elements of the structure tensor\n",
        "Ix = (Iu * Iu).convolve(kgaussian)\n",
        "Iy = (Iv * Iv).convolve(kgaussian)\n",
        "Ixy = (Iu * Iv).convolve(kgaussian)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qS6voZgzu5c"
      },
      "source": [
        "An interest point $(u, v)$ is one where whatever direction we move the window it rapidly becomes dissimilar to the original region. If we consider the original image $\\mathbf{I}$ as a surface the eigenvalues of $\\mathbf{A}$ are the principal curvatures of the surface at that point: \n",
        "\n",
        "* If both eigenvalues are small then the surface is flat, that is the image region has approximately constant local intensity. \n",
        "* If one eigenvalue is high and the other low, then the surface is ridge shaped which indicates an edge. \n",
        "* If both eigenvalues are high the surface is sharply peaked which we consider to be a corner.\n",
        "\n",
        "The well known Shi-Tomasi detector considers the strength of the corner, or cornerness, as the minimum eigenvalue\n",
        "\n",
        "$C_{\\text{ST}}(u,v) = \\min( \\lambda_1, \\lambda_2)$\n",
        "\n",
        "where $\\lambda_i$ are the eigenvalues of $\\mathbf{A}$. Points in the image for which this measure is high are referred to as [\"good features to track\"](http://www.ai.mit.edu/courses/6.891/handouts/shi94good.pdf). \n",
        "\n",
        "The Harris detector is based on this same insight but defines corner strength as\n",
        "\n",
        "$C_{H}(u,v) = \\det(\\mathbf{A}) - k \\text{trace}(\\mathbf{A})$\n",
        "\n",
        "and again a large value represents a strong, distinct, corner. Since $\\det(A) = \\lambda_1  \\lambda_2$ and\n",
        "$\\text{trace}(A) = \\lambda_1 + \\lambda_2$ the Harris detector responds when both eigenvalues are large and elegantly avoids computing the eigenvalues of A which has a somewhat higher computational cost.  Typically $k=0.04$.\n",
        "\n",
        "We compute this for all pixels at once"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBUHD26dzu5c"
      },
      "outputs": [],
      "source": [
        "rawC = (Ix * Iy - Ixy * Ixy) - 0.04 * (Ix + Iy)**2\n",
        "rawC.disp(colormap='signed');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLiSZ45Uzu5c"
      },
      "source": [
        "which shows the raw value of $C_H$. If we zoom in and examine the image we will see that large areas of smooth background are around zero, edges are negative and corners of things (like the letters) have a positive value.\n",
        "\n",
        "Next we need to find the coordinates of those positive values.  For every pixel we find the largest value in a $5\\times 5$ window around each pixel but not including the centre pixel itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URXGVC_vzu5d"
      },
      "outputs": [],
      "source": [
        "# create the structuring element\n",
        "SE = np.ones((5,5), np.uint8)\n",
        "SE[2,2] = 0\n",
        "\n",
        "# find the maximum value around each pixel\n",
        "surrounding_max = rawC.dilate(SE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ8eT2HMzu5d"
      },
      "source": [
        "We've done this using a morphological filtering operation called dilation, you can find more details from the documentation page for [`dilate`](https://docs.opencv.org/master/d4/d86/group__imgproc__filter.html#ga4ff0f3318642c4f469d0e11f242f3b6c).\n",
        "\n",
        "Next we find all the pixels whose value is greater than those surrounding it (which we just computed). This is a common trick in computer vision when we are looking for peaks &ndash; it's called non-local maxima suppresion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6aGCCwBPzu5d"
      },
      "outputs": [],
      "source": [
        "uv = (rawC > surrounding_max).nonzero()  # find the indices of all the peaks\n",
        "peakvals = rawC.image[uv[1,:], uv[0,:]]  # find the peak values\n",
        "k = np.argsort(-peakvals)  # sort those values into descending order\n",
        "print('%d peaks found\\n' % (len(k),))  # report how many peaks were found\n",
        "k = k[:200]  # take first 200, these are the strongest peaks\n",
        "\n",
        "# find the u, v coordinates as two separate lists\n",
        "u = np.array([uv[0][i] for i in k])\n",
        "v = np.array([uv[1][i] for i in k])\n",
        "\n",
        "# display the image, and overlay with markers for each detected corner feature\n",
        "castle.disp(darken=True, block=None);\n",
        "plot_point((u, v), 'y+');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BBbrRIvzu5e"
      },
      "source": [
        "Zoom in on the letters and you will see that the features are placed on sharp corners.\n",
        "\n",
        "### Summary\n",
        "\n",
        "Developed by Chris Harris and Mike Stephens in 1988 the [Harris detector](https://en.wikipedia.org/wiki/Harris_Corner_Detector) was THE detector used in computer vision until\n",
        "[SIFT](https://en.wikipedia.org/wiki/Scale-invariant_feature_transform) was developed by David Lowe in 1999.\n",
        "SIFT is very powerful (it has the very useful property called scale-invariance which we won't have time to cover) but was [patented by the University of British Columbia](https://patents.google.com/patent/US6711293) and researchers were therefore reluctant to be too dependent on it. [SURF](https://en.wikipedia.org/wiki/Speeded_up_robust_features) was  developed by Herbert Bay, Tinne Tuytelaars, and Luc Van Gool and published in 2006. It has similar functionality to SIFT but is much faster.  It turns out that [SURF was also patented](https://worldwide.espacenet.com/patent/search/family/036954920/publication/US2009238460A1?q=pn%3DUS2009238460) but people seemed to worry less about that.  Now the SIFT patent has expired so it is back in favour.  OpenCV ships with SIFT but not SURF."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKETKTF_zu5e"
      },
      "source": [
        "# SIFT features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RBNZgm0zu5e"
      },
      "outputs": [],
      "source": [
        "butterfly = Image.Read('butterfly.jpg')\n",
        "butterfly.disp();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVCIPAtmzu5e"
      },
      "source": [
        "Now we can find SIFT features in this scene"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrTcjTrszu5e"
      },
      "outputs": [],
      "source": [
        "features = butterfly.SIFT()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dp6HbEZBzu5f"
      },
      "source": [
        "which is another type of object that contains data about all the features, it behaves a bit like a list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aw6ha2Oszu5f"
      },
      "outputs": [],
      "source": [
        "len(features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuWcaoCpzu5f"
      },
      "source": [
        "and in this case there are 1055 features found.\n",
        "\n",
        "Each feature (sometimes called keypoints) has a number of characteristics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Qgnd_spzu5f"
      },
      "outputs": [],
      "source": [
        "features[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kf1hXxNezu5f"
      },
      "source": [
        "which are also attributes of the object.  For example the centroid is"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AhMxtoA5zu5f"
      },
      "outputs": [],
      "source": [
        "features[0].p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFLjAzUCzu5f"
      },
      "source": [
        "Unlike the Harris feature, the SIFT feature also has a size (in pixels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uu_rwnXxzu5g"
      },
      "outputs": [],
      "source": [
        "features[0].scale"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZnHEzBfzu5g"
      },
      "source": [
        "as well as a characteristic angle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrBa7fgozu5g"
      },
      "outputs": [],
      "source": [
        "features[0].orientation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MyGyz9azu5g"
      },
      "source": [
        "in degrees.\n",
        "\n",
        "Each feature also strength or response (similar to \"cornerness\" for the Harris detector) that indicates how unique the feature is"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2fGdtV2zu5g"
      },
      "outputs": [],
      "source": [
        "features[0].strength"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWmYXsAszu5g"
      },
      "source": [
        "The first 10 SIFT features are"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_m8xzaqYzu5g"
      },
      "outputs": [],
      "source": [
        "features[:10].table()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsiOyCV-zu5g"
      },
      "source": [
        "and they are essentially in the order they were encountered in the image.  We are often interested in the N strongest features, so we would first sort the features into descending order by strength"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNTWOTL6zu5g"
      },
      "outputs": [],
      "source": [
        "features = features.sort()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSQMy7mlzu5h"
      },
      "source": [
        "and now the first 10 are"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQR_AlsLzu5h"
      },
      "outputs": [],
      "source": [
        "features[:10].table()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2yFAlKCzu5h"
      },
      "source": [
        "We can plot the features, their size and orientation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A151FQJ0zu5h"
      },
      "outputs": [],
      "source": [
        "butterfly.disp(block=None)\n",
        "features.sort(by='scale')[:50].plot(filled=True, alpha=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T56heJfFzu5h"
      },
      "source": [
        "where each feature is denoted by a translucent disk that indicates the position and scale of the feature.  Clearly it has found the distinctive points in the image.\n",
        "\n",
        "**Q: Compute and plot a histogram of the feature scales.**\n",
        "\n",
        "Each feature point has a descriptor which is a summary of the pixel values within the variable size disk surrounding it. This is given by the descriptor which is a vector of 64 floats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfSWo6QUzu5h"
      },
      "outputs": [],
      "source": [
        "features[0].descriptor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6egtJLuzu5i"
      },
      "source": [
        "If we moved the camera to shrink the image by a factor of two the size of all the support region disks would reduce by a factor of two, but the descriptor would remain approximately constant.  If we rotated the camera by 90degrees the characteristic angle of all the features would change by 90degrees but the feature size and the descriptor would remain approximately constant.\n",
        "\n",
        "We can display the orientation, along with the feature size and scale by"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Q9LZvqgzu5i"
      },
      "outputs": [],
      "source": [
        "butterfly.disp(block=None)\n",
        "features.sort(by='scale')[:50].plot(filled=True, alpha=0.5, hand=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx-IVNWyzu5i"
      },
      "source": [
        "where the blue line (like a one-handed clock) indicates the characteristic orientation.  Some features are doubled up, same position and scale, but different orientation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWtuQDZwzu5i"
      },
      "source": [
        "# Finding correspondences between images\n",
        "\n",
        "First we will load two different views of the same scene"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-cdFeY4zu5i"
      },
      "outputs": [],
      "source": [
        "im1 = Image.Read('eiffel-1.png')\n",
        "im2 = Image.Read('eiffel-2.png')\n",
        "Image.Hstack((im1,im2)).disp()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEo8EqT3Qv-x"
      },
      "source": [
        "and then use SIFT to find corresponding features in the two images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "md1S7iLHQweC"
      },
      "outputs": [],
      "source": [
        "f1 = im1.SIFT()\n",
        "f2 = im2.SIFT()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xT7AIsA7zu5i"
      },
      "source": [
        "Then we create a \"*match*\" object and give it the two sets of features.  It finds the best matches and for each match computes a `distance` which is a measure of dissimilarity.  We sort the matches into decreasing similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFT_GbQKzu5j"
      },
      "outputs": [],
      "source": [
        "matches = f1.match(f2)\n",
        "matches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZQtJ7mPzu5j"
      },
      "source": [
        "then plot the best 100 matches.  The `plot` method renders the matches onto the original images and overlays lines that connect corresponding points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jM5XWcKzzu5j"
      },
      "outputs": [],
      "source": [
        "matches[:100].plot('y', alpha=0.6, block=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDIAtRqSzu5j"
      },
      "source": [
        "Zoom in and see how well it has done.\n",
        "\n",
        "Finding corresponding points in a pair of images is critical to a lot of important computer vision problems such as stereo, structure from motion and visual SLAM."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "name": "image-features.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "RVSS",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "181.59375px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
